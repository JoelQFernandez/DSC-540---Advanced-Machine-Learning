{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attrition - Group 6\n",
    "> Abigail Keller (2026918, akelle30@depaul.edu)<br>\n",
    "> Kara Faciszewski (1976775, kfacisze@depaul.edu)<br>\n",
    "> Joel Fernandez (1394698, jferna26@depaul.edu)<br>\n",
    "> Elizabeth Kerrigan (1994142, ekerrig3@depaul.edu)<br>\n",
    "> Srinath V.S. (1979936 , SVANAMAM@depaul.edu)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Abstract](./Final%20Project.ipynb#Abstract)<br>\n",
    "[Introduction](./Final%20Project.ipynb#Introduction)<br>\n",
    "Exploratory Data Analysis\n",
    "* [Data Exploration](./Final%20Project.ipynb#Data-Exploration)<br>\n",
    "* [Statistical Summaries, Data Dictionaries And Data Visualizations](./Final%20Project.ipynb#Statistical-Summaries,-Data-Dictionaries-And-Data-Visualizations)\n",
    "* [Pre-Processing](./Final%20Project.ipynb#Pre-Processing)<br>\n",
    "\n",
    "Research Questions\n",
    "* [Question 1](./Final%20Project.ipynb#Question-1:)<br>\n",
    "* [Question 2](./Final%20Project.ipynb#Question-2:)<br>\n",
    "* [Question 3](./Final%20Project.ipynb#Question-3:)<br>\n",
    "* [Question 4](./Final%20Project.ipynb#Question-4:)<br>\n",
    "\n",
    "[Machine Learning](./Final%20Project.ipynb#Machine-Learning)<br>\n",
    "[Results And Conclustions](./Final%20Project.ipynb#Results-And-Conclusions)<br>\n",
    "[References](./Final%20Project.ipynb#References)<br>\n",
    "\n",
    "Jupyter Notebooks <br>\n",
    "* [Data Exploration](./Final%20Project%2001%20Data%20Exploration.ipynb)<br>\n",
    "* [Pre-Processing](./Final%20Project%2002%20Pre-Proccessing.ipynb)<br>\n",
    "* [K Nearest Neighbors](./Final%20Project%2003.1%20K%20Nearest%20Neighbors.ipynb)<br>\n",
    "* [Naive Bayes](./Final%20Project%2003.2%20Naive%20Bayes.ipynb)<br>\n",
    "* [Logistic Regression](./Final%20Project%2003.3%20Logistic%20Regression.ipynb)<br>\n",
    "* [Decision Tree](./Final%20Project%2003.4%20Decision%20Tree.ipynb)<br>\n",
    "* [Random Forest](./Final%20Project%2003.5%20Random%20Forest.ipynb)<br>\n",
    "* [Adaboost](./Final%20Project%2003.6%20AdaBoost.ipynb)<br>\n",
    "* [Gradient Boosting](./Final%20Project%2003.7%20Gradient%20Boosting.ipynb)<br>\n",
    "* [Perceptron](./Final%20Project%2003.8%20Perceptron.ipynb)<br>\n",
    "* [Principal Component Analysis](./Final%20Project%2004.1%20Principal%20Component%20Analysis.ipynb)<br>\n",
    "* [Kernel Principal Component Analysis](./Final%20Project%2004.2%20Kernel%20Principal%20Component%20Analysis.ipynb)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "It takes a lot of resources to train someone how to do their job well and efficiently. Wherever possible, companies do not want that investment leaving for a different job. This dataset could provide some insights into how to best maintain productive employees in order to conserve the time, energy, and money already poured into this employee. This data looks at things like age, the number of years they’ve been with the company, income, salary percent increases, and years since last promotion to try to paint a picture of whether these variables help explain employee attrition. By learning some possible causes for an employee leaving the company, we can help companies limit employee turnover by considering and ameliorating those causes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "## Data Exploration\n",
    "### Record Counts\n",
    "![Record Count](./Assets/Record_Count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "![Missing Values](./Assets/MissingValues.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshots\n",
    "![Snapshot1](./Assets/Snapshot1.png)\n",
    "![Snapshot2](./Assets/Snapshot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summaries, Data Dictionaries And Data Visualizations\n",
    "### Age:\n",
    "*\tEmployee's Age.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![Age](./Assets/Age.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attrition :\n",
    "*\tEmployee left/leaving the company. \n",
    "*\tCategorical (Nominal).\n",
    "    * Yes\n",
    "    * No\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tClass Label will be dropped from the dataframe.\n",
    "    \n",
    "![Attrition](./Assets/Attrition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BusinessTravel:\n",
    "*\tFrequency of travel.\n",
    "*\tCategorical (Nominal).\n",
    "    * Non-Travel\n",
    "    * Travel_Rarely\n",
    "    * Travel_Frequently\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    * Dummy/OneHot encoding.\n",
    "    \n",
    "![BusinessTravel](./Assets/BusinessTravel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DailyRate:\n",
    "*\tDaily salary rate.\n",
    "*\tNumerical (Continuous).\n",
    "*\tNot normally distributed, appears to be sparse.\n",
    "\n",
    "![DailyRate](./Assets/DailyRate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Department:\n",
    "*\tDepartment name.\n",
    "*\tCategorical (Nominal).\n",
    "    * Human Resources\n",
    "    * Research & Development\n",
    "    * Sales\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tDummy/OneHot encoding.\n",
    "    \n",
    "![Department](./Assets/Department.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistanceFromHome:\n",
    "*\tDistance from home.\n",
    "*\tNumerical (Continuous).\n",
    "*\tNot normally distributed, appears to be skewed towards the right.\n",
    "\n",
    "![DistanceFromHome](./Assets/DistanceFromHome.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education:\n",
    "*\tEducation level.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tThere is somc class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tConvert from Categorical(Ordinal) to Categorical(Nominal)\n",
    "        * 1 = ‘Below College’\n",
    "        * 2 = ‘College’\n",
    "        * 3 = 'Bachelor'\n",
    "        * 4 = 'Master'\n",
    "        * 5 = 'Doctor'\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![Education](./Assets/Education.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EducationField:\n",
    "*\tField of education.\n",
    "*\tCategorical (Nominal).\n",
    "    * Human Resources\n",
    "    * Life Sciences\n",
    "    * Marketing\n",
    "    * Medical\n",
    "    * Other\n",
    "    * Technical Degree\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![EducationField](./Assets/EducationField.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmployeeCount:\n",
    "*\tCount of employee per record.\n",
    "*\tNumerical (Discrete).\n",
    "*\tAlthough the skew value is 0.000, it is not normally distributed.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tWill be dropped from the dataframe.\n",
    "\n",
    "![EmployeeCount](./Assets/EmployeeCount.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmployeeNumber:\n",
    "*\tEmployee ID #.\n",
    "*\tCategorical (Nominal).\n",
    "*\tAlthough the skew value is 0.017, it is not normally distributed.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tIndex, will be dropped from the dataframe.\n",
    "\n",
    "![EmployeeNumber](./Assets/EmployeeNumber.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnvironmentSatisfaction:\n",
    "*\tSatisfaction with the work environment.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tConvert from Categorical(Ordinal) to Categorical(Nominal)\n",
    "        * 1 = 'Low'\n",
    "        * 2 = 'Medium'\n",
    "        * 3 = 'High'\n",
    "        * 4 = 'Very High')\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![EnvironmentSatisfaction](./Assets/EnvironmentSatisfaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender:\n",
    "*\tGender.\n",
    "*\tCategorical (Nominal).\n",
    "    * Male\n",
    "    * Female\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tDummy/OneHot encoding.\n",
    "    \n",
    "![Gender](./Assets/Gender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HourlyRate:\n",
    "*\tHourly salary rate.\n",
    "*\tNumerical (Continuous).\n",
    "*\tNot normally distributed, appears to be sparsely populated.\n",
    "\n",
    "![HourlyRate](./Assets/HourlyRate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JobInvolvement:\n",
    "*\tJob involvement level.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tConvert from Categorical(Ordinal) to Categorical(Nominal)\n",
    "        * 1 = 'Low'\n",
    "        * 2 = 'Medium'\n",
    "        * 3 = 'High'\n",
    "        * 4 = 'Very High'\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![JobInvolvement](./Assets/JobInvolvement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JobLevel:\n",
    "*\tJob level.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![JobLevel](./Assets/JobLevel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JobRole:\n",
    "*\tJob role.\n",
    "*\tCategorical (Nominal).\n",
    "    * Healthcare Representative\n",
    "    * Human Resources\n",
    "    * Laboratory Technician\n",
    "    * Manager\n",
    "    * Manufacturing Director\n",
    "    * Research Director\n",
    "    * Research Scientist\n",
    "    * Sales Executive\n",
    "    * Sales Representative\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![JobRole](./Assets/JobRole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JobSatisfaction:\n",
    "*\tSatisfaction with job.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tConvert from Categorical(Ordinal) to Categorical(Nominal)\n",
    "        * 1 = 'Low'\n",
    "        * 2 = 'Medium'\n",
    "        * 3 = 'High'\n",
    "        * 4 = 'Very High'\n",
    "    *\tDummy/OneHot encoding.\n",
    "    \n",
    "![JobSatisfaction](./Assets/JobSatisfaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaritalStatus:\n",
    "*\tMarital Status.\n",
    "*\tCategorical (Nominal).\n",
    "    * Divorced\n",
    "    * Married\n",
    "    * Single\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tDummy/OneHot encoding.\n",
    "    \n",
    "![MaritalStatus](./Assets/MaritalStatus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonthlyIncome:\n",
    "*\tMonthly income.\n",
    "*\tNumerical (Continuous).\n",
    "*\tNot normally distributed, appears to be skewed to the right.\n",
    "\n",
    "![MonthlyIncome](./Assets/MonthlyIncome.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonthlyRate:\n",
    "*\tMonthly salary rate.\n",
    "*\tNumerical (Continuous).\n",
    "*\tNot normally distributed, appears to be sparsely populated.\n",
    "\n",
    "![MonthlyRate](./Assets/MonthlyRate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumCompaniesWorked:\n",
    "*\tNumber of companies worked for.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![NumCompaniesWorked](./Assets/NumCompaniesWorked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over18:\n",
    "*\tEmployee over 18.\n",
    "*\tCategorical (Nominal).\n",
    "    * Yes\n",
    "    * No\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tWill be dropped from the dataframe.\n",
    "\n",
    "![Over18](./Assets/Over18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverTime:\n",
    "*\tOvertime.\n",
    "*\tCategorical (Nominal).\n",
    "    * Yes\n",
    "    * No\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![OverTime](./Assets/OverTime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PercentSalaryHike:\n",
    "*\tPercent increase in salary.\n",
    "*\tNumerical (Continuous).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![PercentSalaryHike](./Assets/PercentSalaryHike.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PerformanceRating:\n",
    "*\tEmployee performance rating.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tConvert from Categorical(Ordinal) to Categorical(Nominal)\n",
    "        * 1 = 'Low'\n",
    "        * 2 = ‘Good’\n",
    "        * 3 = ‘Excellent’\n",
    "        * 4 = ‘Outstanding’\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![PerformanceRating](./Assets/PerformanceRating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RelationshipSatisfaction:\n",
    "*\tSatisfaction with relationship.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tConvert from Categorical(Ordinal) to Categorical(Nominal)\n",
    "        * 1 = 'Low'\n",
    "        * 2 = 'Medium'\n",
    "        * 3 = 'High'\n",
    "        * 4 = 'Very High'\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![PerformanceRating](./Assets/PerformanceRating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardHours:\n",
    "*\tStandard number of work hours.\n",
    "*\tNumerical (Discrete).\n",
    "*\tAlthough the skew value is 0.000, it is not normally distributed.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tWill be dropped from the dataframe.\n",
    "\n",
    "![StandardHours](./Assets/StandardHours.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StockOptionLevel:\n",
    "*\tStock option level. (0, 1, 2, 3)\n",
    "*\tCategorical (Ordinal).\n",
    "*\tThere is some class imbalance.\n",
    "\n",
    "![StockOptionLevel](./Assets/StockOptionLevel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TotalWorkingYears:\n",
    "*\tNumber of years working.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![TotalWorkingYears](./Assets/TotalWorkingYears.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainingTimesLastYear:\n",
    "*\tNumber of hours training last year.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![TrainingTimesLastYear](./Assets/TrainingTimesLastYear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WorkLifeBalance:\n",
    "*\tBalance between work and life.\n",
    "*\tCategorical (Ordinal).\n",
    "*\tThere is some class imbalance.\n",
    "*\tNeed additional Pre-Processing:\n",
    "    *\tConvert from Categorical(Ordinal) to Categorical(Nominal)\n",
    "        * 1 = 'Bad'\n",
    "        * 2 = 'Good'\n",
    "        * 3 = 'Better'\n",
    "        * 4 = 'Best'\n",
    "    *\tDummy/OneHot encoding.\n",
    "\n",
    "![WorkLifeBalance](./Assets/WorkLifeBalance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YearsAtCompany:\n",
    "*\tNumber of years at the company.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![YearsAtCompany](./Assets/WorkLifeBalance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YearsInCurrentRole:\n",
    "*\tNumber of years in current role.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![YearsInCurrentRole](./Assets/YearsInCurrentRole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YearsSinceLastPromotion:\n",
    "*\tNumber of years since last promotion.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![YearsSinceLastPromotion](./Assets/YearsSinceLastPromotion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YearsWithCurrManager:\n",
    "*\tNumber of years with the current manager.\n",
    "*\tNumerical (Discrete).\n",
    "*\tNot normally distributed, it appears to skew to the right.\n",
    "\n",
    "![YearsWithCurrManager](./Assets/YearsWithCurrManager.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "### Correlation Matrix\n",
    "![Correlation Matrix](./Assets/CorrelationMatrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Correlations\n",
    "![Top Correlations](./Assets/TopCorrelations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing\n",
    "1.  Saved 'Attrition' attribute (Class Label) as Attrition.csv.\n",
    "2.  Normalized Numeric Attributes:\n",
    "\n",
    "    'Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'JobLevel','MonthlyIncome','MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'\n",
    "\n",
    "3.  Converted Categorical(Ordinal) attributes to Categorical(Nominal).    And then performed Dummy / OneHot Encoding:\n",
    "\n",
    "    'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance'\n",
    "\n",
    "4.  Performed Dummy / OneHot Encoding:\n",
    "\n",
    "    'BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime'\n",
    "\n",
    "5.  Dropped Attributes:\n",
    "\n",
    "    'Attrition (Class Label)', 'EmployeeNumber(Index)', 'EmployeeCount', 'Over18', 'StandardHours', <br>'JobLevel(95% Correlation)', 'MonthlyIncome(95% Correlation)'\n",
    "\n",
    "6.  Saved the remaining attributes as HR_Employee.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions\n",
    "### Question 1: \n",
    "### What are the biggest predictors of employee attrition?<br>\n",
    "\n",
    "   **a. Possible analysis:** This can be investigated using supervised learning like decision trees, which will provide us decision points of features driving attrition. We can also look at correlation and logistic regression.\n",
    "\n",
    "   **b. Analysis so far:** Some of the highly predictive variables found include overtime, total working years, daily rate, and age. These variables were largely determined through gradient boosting, decision tree classification, and regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "### Can we get better model performance and understand employee attrition better by performing analysis on subsets of data?<br> \n",
    "\n",
    "**a. Results:** In order to answer this research question, we performed logistic regression and decision tree modeling on subsets of data. The following subsets were used: high versus low earners; male versus female employees, and features within company control versus out of company control.\n",
    "\n",
    "**b. High versus low earners:** The first step in predicting high versus low earners was to create a new feature using the monthly rate variable. High earners were defined as monthly rate above the median, and low earners were defined as monthly rate below the median. Using logistic regression, we predicted attrition for high versus low earners. We found that the model was better able to predict attrition for high earners, with a testing accuracy of 87%, compared to testing accuracy of 83% for low earners. The top predictors of churn for high earners are low job involvement, frequent business travel, and employees with sales roles. The top predictors of churn for low earners are low stock option level, low environment satisfaction, and poor work-life balance.\n",
    "\n",
    "**c. Male versus female attrition:** We predicted attrition separately on subsets of male and female employees to determine if we could more accurately predict attrition for one gender. Using logistic regression and grid search, we found that female attrition can more accurately be predicted (testing accuracy of 90%) compared to testing accuracy of 81% for male employees. Top predictors of churn for females are salary, single marital status, and lab tech roles, and top predictors of churn for males are poor work-life balance and sales roles.\n",
    "\n",
    "**d. Factors within company control:** we first narrowed down the feature set to factors within company control. This included: business travel, salary, environment satisfaction, job level, job role, overtime, training time, stock option level, standard hour, and years since last promotion. We ran our decision tree grid search using this subset of features. We found that the decision tree performs better with a subset of features that are within the company control. Testing accuracy is 85% compared to full feature set testing accuracy of 81%. In addition to performing better, we can get insights from the model about actions the Company can take in order to reduce attrition. The top predictor of churn for company controlled features is Overtime. From there, standard hours and hourly rate are the next deciding features that predict churn. \n",
    "\n",
    "    The Company could take action surrounding these insights. For example, I recommend IBM reduces overall overtime requirements, or, if it is unavoidable, paying better for employees that have to do overtime. Surprisingly, years since last promotion, salary, business travel, and training time does not influence strongly whether or not an employee leaves. This is also good information to know, as IBM could focus more on paying better for overtime than offering employee training or stock options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "### Can we get better model performance and understand employee attrition better by performing PCA/KPCA?\n",
    "\n",
    "**a. Results:** Our objective was to reduce features through linear and nonlinear dimensionality reduction and compare results to our original models. We additionally hoped to further analyze the importance of our variables. The data originally included 34 variables, 10 of which were categorical. Once we added dummy variables for each of these, we had a total of 69 feature columns which led to some instances of overfitting. We first tested PCA to compare accuracy and metrics to our original results. We found that 29 variables accounted for 85% of the variance in the data and 39 variables accounted for 95% of the variance. The performance of each model using PCA is listed below. After testing our models with various components included, there was not a significant change in any of our models. Our decision tree model performed marginally better with a 1% increase in validation accuracy and with an increase in all metrics, but this was still not one of our highest performing models overall. We additionally saw a substantial increase in random forest precision from 6% to 86% but the recall remained notably low at 9%. Kernel PCA did not have any notable increase in performance over PCA with validation accuracy and other metrics generally lower. We then tested kernel PCA and found that a parameter setting of 50 components and rbf kernel was ideal. There was no improvement over PCA or our standard models when testing KPCA.\n",
    "![Research 3 Table](./Assets/Research_3_Table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:  \n",
    "### How would the various classification methods stack up in performance against one another and what methods can be used to improve the performance? \n",
    "### Can we find an ideal balance between bias and variance for each model to avoid under and overfitting? \n",
    "### Which are more informative overall or do certain models answer specific questions better than others? \n",
    "### Which are most interpretable, parsimonious and explainable to our audience.\n",
    "**a. Results:** We implemented various machine learning application models to compare their performance and find which best explained our data. We have found that our logistic regression and decision tree models were most informative in regard to feature selection and finding the most important features when predicting employee attrition. PCA additionally aided in analyzing feature importance and which explained the most variance. A few of our models have higher performance than others and the metrics for each model are listed below. Most of our models had similar training and testing accuracies which implies we are not overfitting the data. The random forest model did show signs of possible overfitting which implies higher variance and lower bias. Our highest performing models included the boosting methods, logistic regression, and Naïve bayes. Validation accuracy for gradient boosting was the highest at 91% with a training accuracy of 93%. Naïve Bayes had the highest recall at 65% but precision was quite low at 37% while KNN had the highest precision at 86% but with extremely low recall at 12%. None of our models were consistently or notably high in both recall and precision but gradient boosting and logistic regression both performed fairly well in comparison, and both were well over 50% in both. Their respective F1-scores were the highest at 61% for gradient boosting and 60% for logistic regression. Gradient boosting, logistic regression and adaboost had the highest balanced accuracies as well. Overall, gradient boosting was the best model due to high accuracy, balanced accuracy, and F1-score with consistent precision and recall. We are additionally confident it was not overfitting the data and was balanced between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "The machine learning applications we implemented include adaboost, KNN, gradient boosting, logistic regression, decision tree, random forest, naive bayes, and perceptron. Our models helped answer various research questions and some were more beneficial than others and the results for each are included below. Logistic regression, decision tree, gradient boosting and principal component analysis helped us analyze the importance of our variables and which were most useful to explaining employee attrition. PCA allowed us to find the variables which explained the most variance in our data. Some of these features included overtime, total working years, pay rate, distance from home, and marital status. Some of our models had higher accuracy while others had higher recall or precision. The best model overall was gradient boosting as it had the highest validation accuracy without overfitting the data and while recall was not the highest, it had decently high recall and precision compared to our other models. Another model which performed very well overall was logistic regression with high validation accuracy and fairly high recall and precision. Naïve Bayes performed well with the highest recall of all models and although it had lower validation accuracy, it had high balanced accuracy comparatively. Below, we will explain the process of building each model including hyperparameter tuning and their confusion matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.  [AdaBoost](./Final%20Project%2003.6%20AdaBoost.ipynb)\n",
    "Comparison of metrics between the default classifier and gridsearch best estimator.<br>\n",
    "\n",
    "Observations:<br>\n",
    "Saw improvements in the following Metrics after applying GridSearchCV: Testing Accuracy, Recall, Balanced Accuracy and F1-Score.\n",
    "![AdaBoost Default](./Assets/adaboost_default.png)\n",
    "\n",
    "\n",
    "![AdaBoost Tuned](./Assets/adaboost_tuned.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. [K Nearest Neighbors](./Final%20Project%2003.1%20K%20Nearest%20Neighbors.ipynb)\n",
    "When performing KNN, various K values and additional parameters were tested. Using a K value of 7 or 9 was found to have the highest testing accuracy and avoided overfitting the data. Additional parameters were tested including weights (uniform and distance) and metrics (manhattan, euclidean, and minkowski). After using grid search CV, the best combination was determined to be a K value of 9 with uniform weight setting and manhattan as the distance metric which produced a training accuracy of 86% and testing of 85%.\n",
    "![KNN Confusion Matrix](./Assets/knn_confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\t[Gradient Boosting](./Final%20Project%2003.7%20Gradient%20Boosting.ipynb)\n",
    "Gradient boosting was utilized and some of the parameters tested include learning rate, number of estimators (boosting stages), and maximum depth. After using grid search CV and testing various combinations, the best parameters included a learning rate of 0.5, max depth of 1, and number of boosting stages equal to 250. The model had a training accuracy of 93% and testing accuracy of 91%.\n",
    "![Gradient Boosting Confusion Matrix](./Assets/gradient_boosting_confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\t[Logistic Regression](./Final%20Project%2003.3%20Logistic%20Regression.ipynb)\n",
    "Logistic regression performed very well with default parameters, as well as grid search. Training accuracy was 90% and testing accuracy 85%.\n",
    "![Logistic Regression](./Assets/logistic_regression_confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\t[Decision Tree](./Final%20Project%2003.4%20Decision%20Tree.ipynb)\n",
    "\n",
    "The base decision tree model overfit the training data with default parameters. Training accuracy was 100% and testing accuracy was 75%. Through grid search, the model reduced bias on the model and improved overall performance. The best model chosen performed at training accuracy of 88% and testing accuracy of 80%.\n",
    "![Decision Tree](./Assets/decision_tree_confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.\t[Random Forest](./Final%20Project%2003.5%20Random%20Forest.ipynb)\n",
    "\n",
    "We saw very little improvements in the model after using the best estimator (min_samples_split=5, n_estimators=250) for the random forest classifier as opposed to the default settings. The differences in balanced accuracy and F1 scores were negligible.\n",
    "\n",
    "![Random Forest Default](./Assets/random_forest_default.png)\n",
    "\n",
    "![Random Forest Default](./Assets/random_forest_tuned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g.\t[Naive Bayes](./Final%20Project%2003.2%20Naive%20Bayes.ipynb)\n",
    "\n",
    "The Naive Bayes algorithm underperformed when compared to all the other algorithms, as its training and testing error was much greater than the other conventional methods on this particular dataset.\n",
    "\n",
    "![Naive Bayes](./Assets/naive_bayes_confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h.\t[Perceptron](./Final%20Project%2003.8%20Perceptron.ipynb)\n",
    "A simple perceptron has been implemented, with a small learning rate. The following data was observed as a result. The perceptron had a reasonable training and testing accuracy.\n",
    "![Perceptron](./Assets/perceptron_confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Of Models\n",
    "![Machine Learning Table](./Assets/Machine_Learning_Table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results And Conclusions\n",
    "   * Were you able to answer your research questions? \n",
    "   * Evaluate the results, were you were expecting these, or were the results contrary to your expectations. Provide some insights. Compare to real-life scenarios or outcomes – if there is any. (Example: You found BMI is negatively correlated with being healthy and based on your regression or classification models as the BMI increases the chance of being unhealthy also increases. Is this consistent to the general knowledge we have or is it quite different?) \n",
    "   * What was the best model based on what metric (error value, or accuracy score of your choice)? \n",
    "   * What more could you do on this data if you had more time? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "*\t[Supervised learning — scikit-learn 0.24.2 documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n",
    "*\t[IBM HR Employee Attrition Dataset](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset)\n",
    "*\t[Qiao, F. (2019, January 8). Logistic Regression Model Tuning with scikit-learn - Part 1. Medium](https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5) \n",
    "*\t[KNN Classifier - scikit-learn 0.24.2](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "*\t[Gradient Boosting Classifier - scikit-learn 0.24.2](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "*\t[AdaBoost Classifier - scikit-learn 0.14](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (04 Homework)",
   "language": "python",
   "name": "pycharm-28976d53"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
